{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning\n",
    "\n",
    "Supervised ML is the task of learning a function that maps an input to an output based on example input-output pairs. \n",
    "Formally, we are given with a set $\\mathcal{D}$ consists of (data,labels) pairs:\n",
    "$$\n",
    "\\mathcal{D} = \\{ (x_i , y_i) \\}_{i=1}^{m}\n",
    "$$\n",
    "where $x_i \\in \\mathcal{X}$ are the datapoints and $y_i \\in \\mathcal{Y}$ are the labels. For simplicity, we assume here that the \"labels space\" $\\mathcal{Y}$ is a finite set  $y_i$ that are discrete, univariate variables, i.e., classification settings. \n",
    "\n",
    "The goal in supervised learning is to **fit** a function $f : \\mathcal{X} \\to \\mathcal{Y}$ such that $f(x_i) =y_i $ for all $i=1,\\dots,m$. \n",
    "\n",
    "\n",
    "Traditionaly, the data points $x_i$ are elements of some *vector space*\\, meaning that, each point can be expressed using a $p$-tuple (vector) of numbers\n",
    "$$\n",
    "x_i = (x_{i1}, x_{i2}, \\dots, x_{ip}) ~,\n",
    "$$\n",
    "hence we have $f : \\mathbb{R}^p \\to \\mathcal{Y}$. \n",
    "However, when working with multi-way data, and time-series data in particular, the situation is different. \n",
    "\n",
    "## The the case of longitudinal sampling\n",
    "\n",
    "To adhere with the empirical results and demonstrations shown in <cite data-footcite=\"mor2021\">Mor et al.</cite>, we restrict the discussion to the case where data points are gathered across multiple timepoints. That is, each sample $x_i$ is in fact, an $n$ by $p$ matrix, where each of the $p$ columns represents a feature and the rows correspond to different timepoints in which the samples were gathered. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dateutil\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.fftpack import dct, idct\n",
    "\n",
    "from itertools import combinations, product\n",
    "\n",
    "import seaborn as sns; sns.set_style(\"ticks\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys \n",
    "sys.path.append('/home/labs/elinav/uria/mprod/')\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import plot_roc_curve, auc\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    ".. footbibliography::"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "mprod",
   "language": "python",
   "name": "mprod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
